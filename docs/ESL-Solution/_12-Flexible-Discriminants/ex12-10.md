---
title: Ex. 12.10
linktitle: Ex 12.10
type: book
date: "2021-01-02T00:00:00+01:00"
# Prev/next pager order (if `docs_section_pager` enabled in `params.toml`)
weight: 1
toc: false
---

???+ question "Ex. 12.10"
    *Kernels and linear discriminant analysis*. Suppose you wish to carry out a linear discriminant analysis (two classes) using a vector of transformations of the input variable $h(x)$. Since $h(x)$ is high-dimensional, you will use a regularized within-class covariance matrix $\bb{W}_h + \gamma\bb{I}$. Show that the model can be estimated using only the inner products $K(x_i, x_{i'})=\langle h(x_i), h(x_{i'}) \rangle$. Hence the kernel property of support vector machines is also shared by regularized linear discriminant analysis.

??? done "Soln. 12.10"
    This problem is two-fold. First, with the kernel $K$, LDA can be solved using the same logic discussed Section 12.3.3 as SVM, or more generally in Section 5.8. See Ex. 18.13 for solving logistic regression as well. 

    Second, the regularized LDA (may arise for high-dimensional problems) share the same logic as normal LDA in finding solutions. See [Ex. 12.6](ex12-06.md) - [Ex. 12.7](ex12-07.md) for comparison in optimal scoring problem (which has the solution proportional to LDA directions). In addition, see Ex. 18.10 for it's connection to the *maximal data piling direction*. 

??? warning "TODO"
    Fix reference when Ex. 18.10 and 18.13 is added.